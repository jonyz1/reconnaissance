import requests
from urllib.parse import urlparse
from bs4 import BeautifulSoup

def check_http_headers(url):
    print("\n[+] Checking security HTTP headers...")
    try:
        r = requests.get(url, timeout=10)
        headers = r.headers
        expected_headers = {
            "Content-Security-Policy": "Mitigates XSS and data injection.",
            "X-Content-Type-Options": "Prevents MIME sniffing.",
            "X-Frame-Options": "Protects against clickjacking.",
            "Strict-Transport-Security": "Enforces HTTPS.",
            "Referrer-Policy": "Controls referrer data exposure.",
            "Permissions-Policy": "Limits access to browser features."
        }

        for header, purpose in expected_headers.items():
            if header in headers:
                print(f"[+] {header} ✅ - Present ({purpose})")
            else:
                print(f"[-] {header} ❌ - Missing ({purpose})")

        if "Set-Cookie" in headers:
            cookie = headers["Set-Cookie"]
            if "HttpOnly" not in cookie:
                print("[-] HttpOnly flag missing from cookies.")
            if "Secure" not in cookie:
                print("[-] Secure flag missing from cookies.")
    except Exception as e:
        print(f"[!] Error checking headers: {e}")

def check_robots_txt(url):
    print("\n[+] Checking for robots.txt and sensitive entries...")
    try:
        parsed = urlparse(url)
        robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
        r = requests.get(robots_url, timeout=10)
        if r.status_code == 200:
            print(f"[+] robots.txt found:")
            lines = r.text.splitlines()
            for line in lines:
                if line.lower().startswith("disallow"):
                    print(f"    {line}")
                    if "admin" in line or "backup" in line or ".git" in line:
                        print("    [-] Warning: Sensitive path may be exposed!")
        else:
            print("[-] robots.txt not found.")
    except Exception as e:
        print(f"[!] Error checking robots.txt: {e}")

def check_directory_listing(url):
    print("\n[+] Checking for directory listing vulnerabilities...")
    try:
        r = requests.get(url, timeout=10)
        soup = BeautifulSoup(r.text, "html.parser")
        title = soup.title.string if soup.title else ""

        if "Index of /" in r.text or "Index of" in title:
            print("[-] Possible directory listing enabled!")
            links = soup.find_all("a")
            for link in links:
                href = link.get("href", "")
                if href and not href.startswith("?") and not href.startswith("#"):
                    print(f"    -> {href}")
        else:
            print("[+] Directory listing not detected.")
    except Exception as e:
        print(f"[!] Error checking directory listing: {e}")

def check_server_version(url):
    print("\n[+] Checking server version...")
    try:
        r = requests.get(url, timeout=10)
        server = r.headers.get("Server", "Not specified")
        print(f"[+] Server header: {server}")
    except Exception as e:
        print(f"[!] Error checking server header: {e}")

def run():
    print("=== Simple Vulnerability Scanner ===")
    target = "https://aii.et"
    check_http_headers(target)
    check_robots_txt(target)
    check_directory_listing(target)
    check_server_version(target)
